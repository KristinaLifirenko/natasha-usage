{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "%run -n main.py\n",
    "USER = 'alexanderkuk'\n",
    "PASS = get_pass()\n",
    "AUTH = (USER, PASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Yargy*\n",
    "- bot parse time https://github.com/asyncee/phophet-bot/blob/master/tests/test_times.py\n",
    "- edu measures https://github.com/anthonyshevchuk/hello-world/blob/master/Extraction_Example_4.ipynb\n",
    "- edu references https://github.com/alexmk7/python_school/tree/master/yargy.ipynb\n",
    "- https://github.com/ilyaly/CrimesParser-MVD_RF-/tree/master/Crimes_to_geojson.py\n",
    "- https://github.com/tsundokum/nkvd_members/blob/master/rules.py\n",
    "- bot https://github.com/agvinogradov/smart_telegram_bot/tree/master/user_manager/yargy_ner.py\n",
    "- https://github.com/kuznetsovn/PZZ/tree/master/4. add coordinates.ipynb\n",
    "- bot https://github.com/Ilyaololo/voice-helper/blob/master/text_catcher/name_entity_extraction/rules.py\n",
    "- bot parse time2 https://github.com/kc41/tg_dobby/tree/master/tg_dobby/grammar \n",
    "\n",
    "*Natasha*\n",
    "- edu ner https://github.com/mannefedov/compling_nlp_hse_course/blob/master/notebooks/NER.ipynb\n",
    "- ? edu names https://github.com/DariaZvereva/Coreference-Resolution/blob/master/Russian/mention_extractor.py\n",
    "- edu demo https://github.com/echernyak/nlp-course-fintech/blob/master/Seminar_NLP_Fintech_20_09.ipynb\n",
    "- ? vk https://github.com/nizhikebinesi/fun_scripts/blob/master/short_posts\n",
    "- comp https://github.com/Samchenko/fupm_4_course/blob/master/surnames_review.ipynb\n",
    "- test dates https://github.com/yutkin/test-task-avito-coins/blob/master/main.py\n",
    "- test https://github.com/TonyKat/test_lda_ne/blob/master/test_ne%2Byargy%2Bnatasha.py\n",
    "- prozhito location https://github.com/euetova/HSE/tree/master/Prozhito\n",
    "- edu https://github.com/creaciond/nlp3/blob/master/seminars/seminar7/NER.ipynb\n",
    "- edu comp https://github.com/svdcvt/minor/blob/master/gazprom_prediction.ipynb\n",
    "- comp https://github.com/KirillTushin/Surnames_classification/blob/master/natasha_pymystem.ipynb\n",
    "- anon https://github.com/bavadim/aninomizer/blob/master/aninomizer.py\n",
    "- edu comp https://github.com/applied-data-science/Data_Mining_in_Action_2018_Spring/blob/master/sport/hw0_surnames_classification/solutions/034_Alexander_Myltsev.ipynb\n",
    "- edu names org https://github.com/anotherbugmaster/nlp_course/blob/master/task_4/task_4.py\n",
    "- crawl shikimori.org https://github.com/Sugakusha/parse_shiki/blob/master/pars.ipynb\n",
    "- edu names https://github.com/SergeyMikhaylov21/compling_homeworks/blob/master/natasha_textblob.py\n",
    "- edu https://github.com/netology-code/ds3-spring-2018/blob/master/5.%20NLP/libs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 397\n"
     ]
    }
   ],
   "source": [
    "q = gh_q(\n",
    "    '\"from yargy\" OR \"import yargy\" OR \"from natasha\" OR \"import natasha\"',\n",
    "    extensions=('py', 'ipynb'),\n",
    ")\n",
    "url = gh_search_code_url(q)\n",
    "response, data = call_gh(url, AUTH, TEXT_MATCH)\n",
    "total = parse_serp_total(data)\n",
    "print('Total:', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page #: 1 To do: 13\n",
      "Page #: 2 To do: 12\n",
      "Page #: 3 To do: 11\n",
      "Page #: 4 To do: 10\n",
      "Page #: 5 To do: 9\n",
      "Page #: 6 To do: 8\n",
      "Page #: 7 To do: 7\n",
      "Page #: 8 To do: 6\n",
      "Ban\n",
      "Page #: 8 To do: 6\n",
      "Page #: 9 To do: 5\n",
      "Page #: 10 To do: 4\n",
      "Page #: 11 To do: 3\n",
      "Page #: 12 To do: 2\n",
      "Page #: 13 To do: 1\n",
      "Page #: 14 To do: 0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "serp = []\n",
    "pages = list(get_pages(total))\n",
    "while pages:\n",
    "    page = pages.pop(0)\n",
    "    print('Page #:', page, 'To do:', len(pages))\n",
    "    url = gh_search_code_url(q, page=page)\n",
    "    response, data = call_gh(url, AUTH, TEXT_MATCH)\n",
    "    if is_broken(data):\n",
    "        print('Ban')\n",
    "        pages.insert(0, page)\n",
    "        sleep(60)\n",
    "    else:\n",
    "        serp.extend(parse_serp(data))\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In cache: 391 New: 0\n"
     ]
    }
   ],
   "source": [
    "cache = set(load_lines(URLS))\n",
    "urls = {get_serp_record_url(_) for _ in serp}\n",
    "print('In cache:', len(cache & urls), 'New:', len(urls - cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'serp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2a65d4ea0371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'(yargy|natasha)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_serp_record_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'serp' is not defined"
     ]
    }
   ],
   "source": [
    "pattern = '(yargy|natasha)'\n",
    "for record in serp:\n",
    "    url = get_serp_record_url(record)\n",
    "    if url in cache:\n",
    "        continue\n",
    "    print(url)\n",
    "    for text in record.matches:\n",
    "        spans = get_spans(text, pattern)\n",
    "        show_markup(text, spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_lines(sorted(cache | urls), URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
